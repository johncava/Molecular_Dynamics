/home/jcava/Molecular_Dynamics/Molecule_Dynamics_GAN/GAN_V12.3.2_Potential_Hierarchy/gan.py:359: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(generator.parameters(), clipping_value)
Traceback (most recent call last):
  File "/home/jcava/Molecular_Dynamics/Molecule_Dynamics_GAN/GAN_V12.3.2_Potential_Hierarchy/gan.py", line 387, in <module>
    gen_frame = generator.generation_step(t, max_generation_steps)
  File "/home/jcava/Molecular_Dynamics/Molecule_Dynamics_GAN/GAN_V12.3.2_Potential_Hierarchy/gan.py", line 142, in generation_step
    z = torch.sigmoid(self.mlp3(z.view(batch_size,6,40)))
RuntimeError: shape '[128, 6, 40]' is invalid for input of size 240
